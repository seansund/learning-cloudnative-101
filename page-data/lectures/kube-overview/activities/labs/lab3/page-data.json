{"componentChunkName":"component---src-pages-lectures-kube-overview-activities-labs-lab-3-index-mdx","path":"/lectures/kube-overview/activities/labs/lab3/","result":{"pageContext":{"frontmatter":{"title":"Kubernetes Lab 3 - Manage Multiple Containers"},"relativePagePath":"/lectures/kube-overview/activities/labs/lab3/index.mdx","titleType":"page","MdxNode":{"id":"02a8882e-06fb-59bb-8968-25cb7f371b7c","children":[],"parent":"c226b8c4-5e07-5e37-b8f6-dfeb87f91bcb","internal":{"content":"---\ntitle: Kubernetes Lab 3 - Manage Multiple Containers\n---\n\n## Problem\n\nThis service has already been packaged into a container image, but there is one special requirement:\n - The legacy app is hard-coded to only serve content on port `8989`, but the team wants to be able to access the service using the standard port `80`.\n\nYour task is to build a Kubernetes pod that runs this legacy container and uses the ambassador design pattern to expose access to the service on port `80`.\n\nThis setup will need to meet the following specifications:\n\n- The pod should have the name `vader-service`.\n- The `vader-service` pod should have a container that runs the legacy vader service image: `ibmcase/millennium-falcon:1`.\n- The `vader-service` pod should have an ambassador container that runs the `haproxy:1.7` image and proxies incoming traffic on port `80` to the legacy service on port `8989` (the HAProxy configuration for this is provided below).\n- Port `80` should be exposed as a `containerPort`.\n\n\n<InlineNotification>\n\n**Note**: You do not need to expose port 8989\n\n</InlineNotification>\n\n- The HAProxy configuration should be stored in a ConfigMap called `vader-service-ambassador-config`.\n- The HAProxy config should be provided to the ambassador container using a volume mount that places the data from the ConfigMap in a file at /usr/local/etc/haproxy/haproxy.cfg.\nhaproxy.cfg should contain the following configuration data:\n\n```\nglobal\n    daemon\n    maxconn 256\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nlisten http-in\n    bind *:80\n    server server1 127.0.0.1:8989 maxconn 32\n```\n\nOnce your pod is up and running, it's a good idea to test it to make sure you can access the service from within the cluster using port 80. In order to do this, you can create a busybox pod in the cluster, and then run a command to attempt to access the service from within the busybox pod.\n\nCreate a descriptor for the busybox pod called `busybox.yml`\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - name: myapp-container\n    image: radial/busyboxplus:curl\n    command: ['sh', '-c', 'while true; do sleep 3600; done']\n```\n\nCreate the busybox testing pod.\n```\nkubectl apply -f busybox.yml\n```\n\nUse this command to access `vader-service` using port 80 from within the busybox pod.\n```\nkubectl exec busybox -- curl $(kubectl get pod vader-service -o=custom-columns=IP:.status.podIP --no-headers):80\n```\n\nIf the service is working, you should get a message that the hyper drive of the millennium falcon needs repair.\n\n*Relevant Documentation:*\n- [Kubernetes Sidecar Logging Agent](https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-sidecar-container-with-the-logging-agent)\n- [Shared Volumes](https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/)\n- [Distributed System Toolkit Patterns](https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/)\n","type":"Mdx","contentDigest":"0909319bdf09f0c892bbba4ffa92f5fe","counter":628,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kubernetes Lab 3 - Manage Multiple Containers\n---\n\n## Problem\n\nThis service has already been packaged into a container image, but there is one special requirement:\n - The legacy app is hard-coded to only serve content on port `8989`, but the team wants to be able to access the service using the standard port `80`.\n\nYour task is to build a Kubernetes pod that runs this legacy container and uses the ambassador design pattern to expose access to the service on port `80`.\n\nThis setup will need to meet the following specifications:\n\n- The pod should have the name `vader-service`.\n- The `vader-service` pod should have a container that runs the legacy vader service image: `ibmcase/millennium-falcon:1`.\n- The `vader-service` pod should have an ambassador container that runs the `haproxy:1.7` image and proxies incoming traffic on port `80` to the legacy service on port `8989` (the HAProxy configuration for this is provided below).\n- Port `80` should be exposed as a `containerPort`.\n\n\n<InlineNotification>\n\n**Note**: You do not need to expose port 8989\n\n</InlineNotification>\n\n- The HAProxy configuration should be stored in a ConfigMap called `vader-service-ambassador-config`.\n- The HAProxy config should be provided to the ambassador container using a volume mount that places the data from the ConfigMap in a file at /usr/local/etc/haproxy/haproxy.cfg.\nhaproxy.cfg should contain the following configuration data:\n\n```\nglobal\n    daemon\n    maxconn 256\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nlisten http-in\n    bind *:80\n    server server1 127.0.0.1:8989 maxconn 32\n```\n\nOnce your pod is up and running, it's a good idea to test it to make sure you can access the service from within the cluster using port 80. In order to do this, you can create a busybox pod in the cluster, and then run a command to attempt to access the service from within the busybox pod.\n\nCreate a descriptor for the busybox pod called `busybox.yml`\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - name: myapp-container\n    image: radial/busyboxplus:curl\n    command: ['sh', '-c', 'while true; do sleep 3600; done']\n```\n\nCreate the busybox testing pod.\n```\nkubectl apply -f busybox.yml\n```\n\nUse this command to access `vader-service` using port 80 from within the busybox pod.\n```\nkubectl exec busybox -- curl $(kubectl get pod vader-service -o=custom-columns=IP:.status.podIP --no-headers):80\n```\n\nIf the service is working, you should get a message that the hyper drive of the millennium falcon needs repair.\n\n*Relevant Documentation:*\n- [Kubernetes Sidecar Logging Agent](https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-sidecar-container-with-the-logging-agent)\n- [Shared Volumes](https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/)\n- [Distributed System Toolkit Patterns](https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/)\n","frontmatter":{"title":"Kubernetes Lab 3 - Manage Multiple Containers"},"fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/learning-cloudnative-101/src/pages/lectures/kube-overview/activities/labs/lab3/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}