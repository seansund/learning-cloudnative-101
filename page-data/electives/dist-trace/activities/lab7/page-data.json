{"componentChunkName":"component---src-pages-electives-dist-trace-activities-lab-7-index-mdx","path":"/electives/dist-trace/activities/lab7/","result":{"pageContext":{"frontmatter":{"title":"Using Jaeger in Service Mesh(Istio) with Open Liberty"},"relativePagePath":"/electives/dist-trace/activities/lab7/index.mdx","titleType":"page","MdxNode":{"id":"7e7af40f-7a04-5679-b3ff-83644f45ccfe","children":[],"parent":"39f79d26-4ba4-5c63-a320-615dadb132b8","internal":{"content":"---\ntitle: Using Jaeger in Service Mesh(Istio) with Open Liberty\n---\n\n### General Instructions\n\n1. Clone the git repository\n\n  ```\n  git clone https://github.com/ibm-cloud-architecture/learning-distributed-tracing-101.git\n  ```\n\n<InlineNotification>\n\n**Important:** You must use the `https` protocol for cloning the repo. If you use the `git` protocol, you will see a build failure in the next steps of the lab and have to clone the repository again with the `https` protocol.\n\n</InlineNotification>\n\n2. Change to the lab directory for **Open Liberty**\n\n  ```\n  cd learning-distributed-tracing-101\n  ```\n\n## Understanding Jaeger, Service Mesh, Kiali\n\nRead the OpenShift Documentation for:\n  * [Understanding Jaeger](https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/ossm-jaeger.html)\n  * [Understanding Service Mesh](https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/understanding-ossm.html)\n  * [Understanding Kiali](https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/ossm-kiali.html)\n\n## Installing Service Mesh (Istio) Operator\n\nThe following operators and tools are installed and configured as part of the Service Mesh installation:\n\n* Jaeger operator \n* Prometheus, Grafana, and Kiali\n\nWith OpenShift 4, you can use the [CodeReady Containers](https://cloud.redhat.com/openshift/install/crc/installer-provisioned) to set up a local development cluster \n\n* [Preparing to install Red Hat OpenShift Service Mesh](https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_install/preparing-ossm-installation.html)\n* [Installing Red Hat OpenShift Service Mesh](https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_install/installing-ossm.html)\n* [Familiarize yourself with OpenShift command-line and console](https://learn.openshift.com/introduction/getting-started/)\n\n## Verify Service Mesh installation\n\n3. Verify that istio components are installed in the namespace `istio-system`\n\n  ```\n  oc get pods -n istio-system\n  ```\n\nVerify the output:\n\n  ```\n  NAME                                      READY   STATUS    RESTARTS   AGE\n  grafana-57dbfb688d-8rkzm                  2/2     Running   0          61m\n  istio-citadel-54f4c55c67-4djdw            1/1     Running   0          65m\n  istio-egressgateway-767484c77f-zcbp5      1/1     Running   0          61m\n  istio-galley-7cbcb5bd98-qzzbg             1/1     Running   0          63m\n  istio-ingressgateway-6dbdc4dbdc-lzxfm     1/1     Running   0          61m\n  istio-pilot-5f5c7dd5b4-nbqsd              2/2     Running   0          62m\n  istio-policy-768ff8c77-qpb4j              2/2     Running   0          63m\n  istio-sidecar-injector-6f5686f954-xlmdv   1/1     Running   0          61m\n  istio-telemetry-64d99945dc-rn5xv          2/2     Running   0          63m\n  jaeger-57776787bc-nd4sg                   2/2     Running   0          63m\n  kiali-549ccd69f4-v2rsv                    1/1     Running   0          56m\n  prometheus-797855d5cf-wdmct               2/2     Running   0          65m\n  ```\n\n4. Verify services in the namespace `istio-system`\n\n  ```\n  oc get services -n istio-system\n  ```\n\nVerify the output:\n\n  ```\n  NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                  AGE\n  grafana                     ClusterIP   172.30.28.190    <none>        3000/TCP                                 61m\n  istio-citadel               ClusterIP   172.30.120.72    <none>        8060/TCP,15014/TCP                       65m\n  istio-egressgateway         ClusterIP   172.30.147.31    <none>        80/TCP,443/TCP,15443/TCP                 62m\n  istio-galley                ClusterIP   172.30.218.118   <none>        443/TCP,15014/TCP,9901/TCP               64m\n  istio-ingressgateway        ClusterIP   172.30.35.42     <none>        15020/TCP,80/TCP,443/TCP,15443/TCP       62m\n  istio-pilot                 ClusterIP   172.30.225.60    <none>        15010/TCP,15011/TCP,8080/TCP,15014/TCP   62m\n  istio-policy                ClusterIP   172.30.157.199   <none>        9091/TCP,15004/TCP,15014/TCP             63m\n  istio-sidecar-injector      ClusterIP   172.30.155.62    <none>        443/TCP                                  61m\n  istio-telemetry             ClusterIP   172.30.118.27    <none>        9091/TCP,15004/TCP,15014/TCP,42422/TCP   63m\n  jaeger-agent                ClusterIP   None             <none>        5775/TCP,5778/TCP,6831/TCP,6832/TCP      64m\n  jaeger-collector            ClusterIP   172.30.172.121   <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   64m\n  jaeger-collector-headless   ClusterIP   None             <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   64m\n  jaeger-query                ClusterIP   172.30.81.233    <none>        443/TCP                                  64m\n  kiali                       NodePort    172.30.129.56    <none>        20001:30998/TCP                          60m\n  prometheus                  ClusterIP   172.30.153.80    <none>        9090/TCP                                 65m\n  zipkin                      ClusterIP   172.30.102.53    <none>        9411/TCP                                 64m\n  ```\n\n5. Verify that the ServiceMeshMemberRoll includes the target namespace for example `default` as one of the `MEMBERS`\n\n  ```\n  oc get ServiceMeshMemberRoll -n istio-system\n  NAME      MEMBERS\n  default   [default bookinfo]\n  ```\n\n6. Verify routes to the different UI dashboards for Jaeger, Grafana, and Kiali\n\n  ```\n  oc get route -n istio-system\n  ```\n\nVerify the output, make sure `jaeger-query` is using `edge` for tls termination, if not you can use `oc edit service jaeger-query -n istio-system` and change it.\n\n  ```\n  NAME                   HOST/PORT                                            PATH   SERVICES               PORT    TERMINATION   WILDCARD\n  grafana                grafana-istio-system.apps-crc.testing                       grafana                <all>   reencrypt     None\n  istio-ingressgateway   istio-ingressgateway-istio-system.apps-crc.testing          istio-ingressgateway   8080                  None\n  jaeger                 jaeger-istio-system.apps-crc.testing                        jaeger-query           <all>   edge          None\n  kiali                  kiali-istio-system.apps-crc.testing                         kiali                  <all>   reencrypt     None\n  prometheus             prometheus-istio-system.apps-crc.testing                    prometheus             <all>   reencrypt     None\n  ```\n\nOpen the different UIs in the browser using the route's values for HOST/PORT\n  * Jaeger: https://jaeger-istio-system.apps-crc.testing\n  * Grafana: https://grafana-istio-system.apps-crc.testing\n  * Kiali: https://kiali-istio-system.apps-crc.testing\n\n## Build the Applications\n\nThe next step is to build the applications inside OpenShift so that they become available in the OpenShift registry for the deployment in the next section:\n\n  ```\n  # still using the \"learning-distributed-tracing-101\" directory\n  oc new-build . --strategy=docker --context-dir=lab-jaeger-istio-ol/service-a --name service-a-openliberty-istio\n  ```\n\nYou can follow the build progress through the OpenShift console from the \"Developer\" perspective, then clicking on \"Builds\" and selecting the corresponding build name (`service-a`), then selecting the `Logs` tab.\n\nYou can also follow the build progress via command-line, with this command:\n\n  ```\n  oc logs -f bc/service-a-openliberty-istio\n  ```\n\nYou should see the following message upon build completion:\n\n  ```\n  ...\n  Writing manifest to image destination\n  Storing signatures\n  Successfully pushed image-registry.openshift-image-registry.svc:5000/default/service-a-openliberty-istio@sha256:14dc4b440e94066818d1ac9d4b06132d61c61a347c5230971159e059c9adf5de\n  Push successful\n  ```\n\n<InlineNotification>\n\n*Important*: If you accidentally cloned the Git repository using the `git` protocol, you will see error messages in the build log that are similar to these:\n\n</InlineNotification>\n\n\n  ```\n  Cloning \"git@github.com:ibm-cloud-architecture/learning-distributed-tracing-101.git\" ...\n  error: Host key verification failed.\n  fatal: Could not read from remote repository.\n\n  Please make sure you have the correct access rights\n  and the repository exists.\n  ```\n\nTo recover from that error condition, delete the local clone and clone the repository again using the `https` protocol. Once the clone is complete, delete the build configuration objects created by the `oc new-build` command by entering the `oc delete` commands below and then repeat the `oc new-build` step:\n\n  ```sh\n  oc delete buildconfig service-a-openliberty-istio -n default\n  oc delete imagestream open-liberty -n default\n  ```\n\nNote that you can safely ignore these warning messages in the build log:\n\n```\ntime=\"2020-03-18T19:15:00Z\" level=warning msg=\"pkg/chroot: error unmounting \\\"/tmp/buildah888423814/mnt/rootfs\\\": error checking if \\\"/tmp/buildah888423814/mnt/rootfs/sys/fs/cgroup/cpuset\\\" is mounted: no such file or directory\"\n```\n\nNow build `service-b`:\n\n  ```\n  # still using the \"learning-distributed-tracing-101\" directory\n  oc new-build . --strategy=docker --context-dir=lab-jaeger-istio-ol/service-b --name service-b-openliberty-istio\n  ```\n\nOnce again, you can follow the build progress via OpenShift console or observing the build logs with the following command:\n\n  ```\n  oc logs -f bc/service-b-openliberty-istio\n  ```\n\nAfter both builds are completed, proceed to deploy the application.\n\n\n## Deploy the Applications\n\n1. Deploy the services `service-a` and `service-b`\n\nUse the file `istio-openliberty.yaml` for Java\n\nHere is an example:\n\n  ```\n  cd lab-jaeger-istio-ol\n  oc apply -f istio-openliberty.yaml -n default\n  ```\n\nLet's look at the file content on how the services are defined to be deployed into OpenShift cluster:\n\n```\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-a\n  labels:\n    app: service-a\nspec:\n  ports:\n    - port: 9080\n      name: http\n  selector:\n    app: service-a\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-a\n  labels:\n    app: service-a\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-a\n  template:\n    metadata:\n      labels:\n        app: service-a\n        version: v1\n      annotations:\n        sidecar.istio.io/inject: \"true\"\n    spec:\n      containers:\n        - name: app\n          image: image-registry.openshift-image-registry.svc:5000/default/service-a-openliberty-istio\n          env:\n            - name: JAEGER_ENDPOINT\n              value: http://jaeger-collector.istio-system.svc:14268/api/traces\n            - name: JAEGER_PROPAGATION\n              value: b3\n            - name: SERVICE_FORMATTER\n              value: service-b\n            - name: JAEGER_REPORTER_LOG_SPANS\n              value: \"true\"\n            - name: JAEGER_SAMPLER_TYPE\n              value: const\n            - name: JAEGER_SAMPLER_PARAM\n              value: \"1\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 9080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-b\n  labels:\n    app: service-b\nspec:\n  ports:\n    - port: 9081\n      name: http\n  selector:\n    app: service-b\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-b\n  labels:\n    app: service-b\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-b\n  template:\n    metadata:\n      labels:\n        app: service-b\n        version: v1\n      annotations:\n        sidecar.istio.io/inject: \"true\"\n    spec:\n      containers:\n        - name: app\n          image: image-registry.openshift-image-registry.svc:5000/default/service-b-openliberty-istio\n          env:\n            - name: JAEGER_ENDPOINT\n              value: http://jaeger-collector.istio-system.svc:14268/api/traces\n            - name: JAEGER_PROPAGATION\n              value: b3\n            - name: JAEGER_REPORTER_LOG_SPANS\n              value: \"true\"\n            - name: JAEGER_SAMPLER_TYPE\n              value: const\n            - name: JAEGER_SAMPLER_PARAM\n              value: \"1\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 9081\n```\n\nIn the YAML deployment manifest there are few items to point out:\n\n**Ports**\n  * The port for the container is specified in the service and the container in the deployment, for example, `service-a` with port `9080` and `service-b` with port `9081`\n\n**Environment Variables**\n  * The variable `JAEGER_ENDPOINT` is specified to indicate to the Jaeger client library to send the traces using http to the jaeger collector service `http://jaeger-collector.istio-system.svc:14268/api/traces` that is deployed on the namespace `istio-system`. \n  * The variable `SERVICE_FORMATTER` used by `service-a` to indicate the hostname of `service-b` that will use to format the hello message.\n  * The variable `JAEGER_PROPAGATION` is set to `b3` this is necessary because the Envoy proxy does not recognize Jaeger's default on-the-wire representation of the trace context, but it does recognize Zipkin's B3 headers. This configuration instructs the Jaeger tracer to use B3 headers instead of its default ones.\n  * The variable `JAEGER_REPORTER_LOG_SPANS` is set to \"true\". It instructs the Jaeger reporter to log finished span IDs. The reporter may need to be given a Logger for this option to take effect.\n  * The variable `JAEGER_SAMPLER_TYPE` is set to `const`, which indicates the constant sampling pattern, as defined [here](https://www.jaegertracing.io/docs/1.17/client-libraries/#sampling).\n  * The variable `JAEGER_SAMPLER_PARAM` is set to 1, which in combination with the constant sampling pattern, means 100% of the spans will be reported to the Jaeger backend.\n\n* Istio has certain [specific requirements](https://istio.io/docs/setup/additional-setup/requirements/). The ones we used in our  `YAML` manifest are the following: **\n  * **Named service ports**\n    * The service port name starts with `http`\n  * **Deployment with app and version labels**\n    * The Pod template should have the following labels defined `app` and `version`\n\n\n2. The `pom.xml` for each service contains the Jaeger client dependency, which can also handle the headers generated by the Istio Envoy proxy forwards, thus allowing for end to end propagation. The source code is available in their respective directories `service-a` and `service-b`, the dependency related to OpenTracing in the file `pom.xml` for the service looks like this:\n\n  ```\n  <dependency>\n      <groupId>io.jaegertracing</groupId>\n      <artifactId>jaeger-client</artifactId>\n      <version>0.34.0</version>\n  </dependency>\n  ```\n\n3. Deploy the Istio Gateway and VirtualService\n\n  ```\n  oc apply -f gateway.yaml -n default\n  ```\n\nHere is the content of `gateway.yaml`\n\n```\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: distributing-tracing-gateway\nspec:\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: distributing-tracing\nspec:\n  hosts:\n    - \"*\"\n  gateways:\n    - distributing-tracing-gateway\n  http:\n    - match:\n        - uri:\n            prefix: /sayHello\n      route:\n        - destination:\n            host: service-a\n            port:\n              number: 9080\n```\n\n4. Verify services are deployed and running:\n\n```\noc get all -l app=service-a -n default\noc get all -l app=service-b -n default\nNAME                             READY   STATUS    RESTARTS   AGE\npod/service-a-799d4dc5f8-v7l74   2/2     Running   0          19m\npod/service-b-5c45ff88d-dr7cl   2/2     Running   0          23m\n\nNAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/service-a   ClusterIP   172.30.243.210   <none>        9080/TCP   19m\nservice/service-b   ClusterIP   172.30.40.248   <none>        9081/TCP   23m\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/service-a   1/1     1            1           19m\ndeployment.apps/service-b   1/1     1            1           23m\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/service-a-799d4dc5f8   1         1         1       19m\nreplicaset.apps/service-b-5c45ff88d   1         1         1       23m\n```\n\nNotice that under the `READY` column for pods, it shows that there are two (2/2) containers running, one of them is the istio sidecar proxy.\n\n5. Get the hostname for the Istio ingress gateway\n\n```\noc get route -n istio-system istio-ingressgateway \nNAME                   HOST/PORT                                            PATH   SERVICES               PORT   TERMINATION   WILDCARD\nistio-ingressgateway   istio-ingressgateway-istio-system.apps-crc.testing          istio-ingressgateway   8080                 None\n```\n\n. Use curl or open a browser with the endpoint URL using the HOST/PORT of the route\n\n```\ncurl http://istio-ingressgateway-istio-system.apps-crc.testing/sayHello/Carlos\n```\n\nNotice in the output that the message was formatted by service-b\n\n```\nHello, from service-b Carlos!\n```\n\nFrom the result, you can see that `service-a` calls `service-b` and replies back.\n\n6. In the Jaeger UI select `istio-ingressgateway` or `service-a` and click **Find Traces**\n\n![istio-trace](../../images/istio-ol-jaeger-traces.png)\n\nYou can see 7 Spans in a single trace starting from the `istio-ingressgateway` ending in `service-b.default`\n\n7. Click on one of the traces and expand the spans in the trace\n\n![istio-spans](../../images/istio-ol-jaeger-spans.png)\n\nCheck one of the labs [Lab Jaeger - Node.js](./lab1) or [Lab Jaeger - Open Liberty](./lab3) for a more in-depth lab for Opentracing with Jaeger.\n\n8. In the Kiali UI select Graph to see a topology view of the services, you can enable traffic animation under Display to see the flow of http requests\n\n![istio-kiali](../../images/istio-ol-kiali.png)\n\n9. In the Grafana UI select the Dashboard *Istio Workload Dashboard* or *Istio Service Dashboard* to see monitoring and metrics data for your services\n\n![istio-grafana](../../images/istio-ol-grafana.png)","type":"Mdx","contentDigest":"ec056e2dbaa37815047e84c95cf9cf28","counter":595,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Using Jaeger in Service Mesh(Istio) with Open Liberty\n---\n\n### General Instructions\n\n1. Clone the git repository\n\n  ```\n  git clone https://github.com/ibm-cloud-architecture/learning-distributed-tracing-101.git\n  ```\n\n<InlineNotification>\n\n**Important:** You must use the `https` protocol for cloning the repo. If you use the `git` protocol, you will see a build failure in the next steps of the lab and have to clone the repository again with the `https` protocol.\n\n</InlineNotification>\n\n2. Change to the lab directory for **Open Liberty**\n\n  ```\n  cd learning-distributed-tracing-101\n  ```\n\n## Understanding Jaeger, Service Mesh, Kiali\n\nRead the OpenShift Documentation for:\n  * [Understanding Jaeger](https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/ossm-jaeger.html)\n  * [Understanding Service Mesh](https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/understanding-ossm.html)\n  * [Understanding Kiali](https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/ossm-kiali.html)\n\n## Installing Service Mesh (Istio) Operator\n\nThe following operators and tools are installed and configured as part of the Service Mesh installation:\n\n* Jaeger operator \n* Prometheus, Grafana, and Kiali\n\nWith OpenShift 4, you can use the [CodeReady Containers](https://cloud.redhat.com/openshift/install/crc/installer-provisioned) to set up a local development cluster \n\n* [Preparing to install Red Hat OpenShift Service Mesh](https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_install/preparing-ossm-installation.html)\n* [Installing Red Hat OpenShift Service Mesh](https://docs.openshift.com/container-platform/4.1/service_mesh/service_mesh_install/installing-ossm.html)\n* [Familiarize yourself with OpenShift command-line and console](https://learn.openshift.com/introduction/getting-started/)\n\n## Verify Service Mesh installation\n\n3. Verify that istio components are installed in the namespace `istio-system`\n\n  ```\n  oc get pods -n istio-system\n  ```\n\nVerify the output:\n\n  ```\n  NAME                                      READY   STATUS    RESTARTS   AGE\n  grafana-57dbfb688d-8rkzm                  2/2     Running   0          61m\n  istio-citadel-54f4c55c67-4djdw            1/1     Running   0          65m\n  istio-egressgateway-767484c77f-zcbp5      1/1     Running   0          61m\n  istio-galley-7cbcb5bd98-qzzbg             1/1     Running   0          63m\n  istio-ingressgateway-6dbdc4dbdc-lzxfm     1/1     Running   0          61m\n  istio-pilot-5f5c7dd5b4-nbqsd              2/2     Running   0          62m\n  istio-policy-768ff8c77-qpb4j              2/2     Running   0          63m\n  istio-sidecar-injector-6f5686f954-xlmdv   1/1     Running   0          61m\n  istio-telemetry-64d99945dc-rn5xv          2/2     Running   0          63m\n  jaeger-57776787bc-nd4sg                   2/2     Running   0          63m\n  kiali-549ccd69f4-v2rsv                    1/1     Running   0          56m\n  prometheus-797855d5cf-wdmct               2/2     Running   0          65m\n  ```\n\n4. Verify services in the namespace `istio-system`\n\n  ```\n  oc get services -n istio-system\n  ```\n\nVerify the output:\n\n  ```\n  NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                  AGE\n  grafana                     ClusterIP   172.30.28.190    <none>        3000/TCP                                 61m\n  istio-citadel               ClusterIP   172.30.120.72    <none>        8060/TCP,15014/TCP                       65m\n  istio-egressgateway         ClusterIP   172.30.147.31    <none>        80/TCP,443/TCP,15443/TCP                 62m\n  istio-galley                ClusterIP   172.30.218.118   <none>        443/TCP,15014/TCP,9901/TCP               64m\n  istio-ingressgateway        ClusterIP   172.30.35.42     <none>        15020/TCP,80/TCP,443/TCP,15443/TCP       62m\n  istio-pilot                 ClusterIP   172.30.225.60    <none>        15010/TCP,15011/TCP,8080/TCP,15014/TCP   62m\n  istio-policy                ClusterIP   172.30.157.199   <none>        9091/TCP,15004/TCP,15014/TCP             63m\n  istio-sidecar-injector      ClusterIP   172.30.155.62    <none>        443/TCP                                  61m\n  istio-telemetry             ClusterIP   172.30.118.27    <none>        9091/TCP,15004/TCP,15014/TCP,42422/TCP   63m\n  jaeger-agent                ClusterIP   None             <none>        5775/TCP,5778/TCP,6831/TCP,6832/TCP      64m\n  jaeger-collector            ClusterIP   172.30.172.121   <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   64m\n  jaeger-collector-headless   ClusterIP   None             <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   64m\n  jaeger-query                ClusterIP   172.30.81.233    <none>        443/TCP                                  64m\n  kiali                       NodePort    172.30.129.56    <none>        20001:30998/TCP                          60m\n  prometheus                  ClusterIP   172.30.153.80    <none>        9090/TCP                                 65m\n  zipkin                      ClusterIP   172.30.102.53    <none>        9411/TCP                                 64m\n  ```\n\n5. Verify that the ServiceMeshMemberRoll includes the target namespace for example `default` as one of the `MEMBERS`\n\n  ```\n  oc get ServiceMeshMemberRoll -n istio-system\n  NAME      MEMBERS\n  default   [default bookinfo]\n  ```\n\n6. Verify routes to the different UI dashboards for Jaeger, Grafana, and Kiali\n\n  ```\n  oc get route -n istio-system\n  ```\n\nVerify the output, make sure `jaeger-query` is using `edge` for tls termination, if not you can use `oc edit service jaeger-query -n istio-system` and change it.\n\n  ```\n  NAME                   HOST/PORT                                            PATH   SERVICES               PORT    TERMINATION   WILDCARD\n  grafana                grafana-istio-system.apps-crc.testing                       grafana                <all>   reencrypt     None\n  istio-ingressgateway   istio-ingressgateway-istio-system.apps-crc.testing          istio-ingressgateway   8080                  None\n  jaeger                 jaeger-istio-system.apps-crc.testing                        jaeger-query           <all>   edge          None\n  kiali                  kiali-istio-system.apps-crc.testing                         kiali                  <all>   reencrypt     None\n  prometheus             prometheus-istio-system.apps-crc.testing                    prometheus             <all>   reencrypt     None\n  ```\n\nOpen the different UIs in the browser using the route's values for HOST/PORT\n  * Jaeger: https://jaeger-istio-system.apps-crc.testing\n  * Grafana: https://grafana-istio-system.apps-crc.testing\n  * Kiali: https://kiali-istio-system.apps-crc.testing\n\n## Build the Applications\n\nThe next step is to build the applications inside OpenShift so that they become available in the OpenShift registry for the deployment in the next section:\n\n  ```\n  # still using the \"learning-distributed-tracing-101\" directory\n  oc new-build . --strategy=docker --context-dir=lab-jaeger-istio-ol/service-a --name service-a-openliberty-istio\n  ```\n\nYou can follow the build progress through the OpenShift console from the \"Developer\" perspective, then clicking on \"Builds\" and selecting the corresponding build name (`service-a`), then selecting the `Logs` tab.\n\nYou can also follow the build progress via command-line, with this command:\n\n  ```\n  oc logs -f bc/service-a-openliberty-istio\n  ```\n\nYou should see the following message upon build completion:\n\n  ```\n  ...\n  Writing manifest to image destination\n  Storing signatures\n  Successfully pushed image-registry.openshift-image-registry.svc:5000/default/service-a-openliberty-istio@sha256:14dc4b440e94066818d1ac9d4b06132d61c61a347c5230971159e059c9adf5de\n  Push successful\n  ```\n\n<InlineNotification>\n\n*Important*: If you accidentally cloned the Git repository using the `git` protocol, you will see error messages in the build log that are similar to these:\n\n</InlineNotification>\n\n\n  ```\n  Cloning \"git@github.com:ibm-cloud-architecture/learning-distributed-tracing-101.git\" ...\n  error: Host key verification failed.\n  fatal: Could not read from remote repository.\n\n  Please make sure you have the correct access rights\n  and the repository exists.\n  ```\n\nTo recover from that error condition, delete the local clone and clone the repository again using the `https` protocol. Once the clone is complete, delete the build configuration objects created by the `oc new-build` command by entering the `oc delete` commands below and then repeat the `oc new-build` step:\n\n  ```sh\n  oc delete buildconfig service-a-openliberty-istio -n default\n  oc delete imagestream open-liberty -n default\n  ```\n\nNote that you can safely ignore these warning messages in the build log:\n\n```\ntime=\"2020-03-18T19:15:00Z\" level=warning msg=\"pkg/chroot: error unmounting \\\"/tmp/buildah888423814/mnt/rootfs\\\": error checking if \\\"/tmp/buildah888423814/mnt/rootfs/sys/fs/cgroup/cpuset\\\" is mounted: no such file or directory\"\n```\n\nNow build `service-b`:\n\n  ```\n  # still using the \"learning-distributed-tracing-101\" directory\n  oc new-build . --strategy=docker --context-dir=lab-jaeger-istio-ol/service-b --name service-b-openliberty-istio\n  ```\n\nOnce again, you can follow the build progress via OpenShift console or observing the build logs with the following command:\n\n  ```\n  oc logs -f bc/service-b-openliberty-istio\n  ```\n\nAfter both builds are completed, proceed to deploy the application.\n\n\n## Deploy the Applications\n\n1. Deploy the services `service-a` and `service-b`\n\nUse the file `istio-openliberty.yaml` for Java\n\nHere is an example:\n\n  ```\n  cd lab-jaeger-istio-ol\n  oc apply -f istio-openliberty.yaml -n default\n  ```\n\nLet's look at the file content on how the services are defined to be deployed into OpenShift cluster:\n\n```\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-a\n  labels:\n    app: service-a\nspec:\n  ports:\n    - port: 9080\n      name: http\n  selector:\n    app: service-a\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-a\n  labels:\n    app: service-a\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-a\n  template:\n    metadata:\n      labels:\n        app: service-a\n        version: v1\n      annotations:\n        sidecar.istio.io/inject: \"true\"\n    spec:\n      containers:\n        - name: app\n          image: image-registry.openshift-image-registry.svc:5000/default/service-a-openliberty-istio\n          env:\n            - name: JAEGER_ENDPOINT\n              value: http://jaeger-collector.istio-system.svc:14268/api/traces\n            - name: JAEGER_PROPAGATION\n              value: b3\n            - name: SERVICE_FORMATTER\n              value: service-b\n            - name: JAEGER_REPORTER_LOG_SPANS\n              value: \"true\"\n            - name: JAEGER_SAMPLER_TYPE\n              value: const\n            - name: JAEGER_SAMPLER_PARAM\n              value: \"1\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 9080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-b\n  labels:\n    app: service-b\nspec:\n  ports:\n    - port: 9081\n      name: http\n  selector:\n    app: service-b\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-b\n  labels:\n    app: service-b\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-b\n  template:\n    metadata:\n      labels:\n        app: service-b\n        version: v1\n      annotations:\n        sidecar.istio.io/inject: \"true\"\n    spec:\n      containers:\n        - name: app\n          image: image-registry.openshift-image-registry.svc:5000/default/service-b-openliberty-istio\n          env:\n            - name: JAEGER_ENDPOINT\n              value: http://jaeger-collector.istio-system.svc:14268/api/traces\n            - name: JAEGER_PROPAGATION\n              value: b3\n            - name: JAEGER_REPORTER_LOG_SPANS\n              value: \"true\"\n            - name: JAEGER_SAMPLER_TYPE\n              value: const\n            - name: JAEGER_SAMPLER_PARAM\n              value: \"1\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 9081\n```\n\nIn the YAML deployment manifest there are few items to point out:\n\n**Ports**\n  * The port for the container is specified in the service and the container in the deployment, for example, `service-a` with port `9080` and `service-b` with port `9081`\n\n**Environment Variables**\n  * The variable `JAEGER_ENDPOINT` is specified to indicate to the Jaeger client library to send the traces using http to the jaeger collector service `http://jaeger-collector.istio-system.svc:14268/api/traces` that is deployed on the namespace `istio-system`. \n  * The variable `SERVICE_FORMATTER` used by `service-a` to indicate the hostname of `service-b` that will use to format the hello message.\n  * The variable `JAEGER_PROPAGATION` is set to `b3` this is necessary because the Envoy proxy does not recognize Jaeger's default on-the-wire representation of the trace context, but it does recognize Zipkin's B3 headers. This configuration instructs the Jaeger tracer to use B3 headers instead of its default ones.\n  * The variable `JAEGER_REPORTER_LOG_SPANS` is set to \"true\". It instructs the Jaeger reporter to log finished span IDs. The reporter may need to be given a Logger for this option to take effect.\n  * The variable `JAEGER_SAMPLER_TYPE` is set to `const`, which indicates the constant sampling pattern, as defined [here](https://www.jaegertracing.io/docs/1.17/client-libraries/#sampling).\n  * The variable `JAEGER_SAMPLER_PARAM` is set to 1, which in combination with the constant sampling pattern, means 100% of the spans will be reported to the Jaeger backend.\n\n* Istio has certain [specific requirements](https://istio.io/docs/setup/additional-setup/requirements/). The ones we used in our  `YAML` manifest are the following: **\n  * **Named service ports**\n    * The service port name starts with `http`\n  * **Deployment with app and version labels**\n    * The Pod template should have the following labels defined `app` and `version`\n\n\n2. The `pom.xml` for each service contains the Jaeger client dependency, which can also handle the headers generated by the Istio Envoy proxy forwards, thus allowing for end to end propagation. The source code is available in their respective directories `service-a` and `service-b`, the dependency related to OpenTracing in the file `pom.xml` for the service looks like this:\n\n  ```\n  <dependency>\n      <groupId>io.jaegertracing</groupId>\n      <artifactId>jaeger-client</artifactId>\n      <version>0.34.0</version>\n  </dependency>\n  ```\n\n3. Deploy the Istio Gateway and VirtualService\n\n  ```\n  oc apply -f gateway.yaml -n default\n  ```\n\nHere is the content of `gateway.yaml`\n\n```\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: distributing-tracing-gateway\nspec:\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: distributing-tracing\nspec:\n  hosts:\n    - \"*\"\n  gateways:\n    - distributing-tracing-gateway\n  http:\n    - match:\n        - uri:\n            prefix: /sayHello\n      route:\n        - destination:\n            host: service-a\n            port:\n              number: 9080\n```\n\n4. Verify services are deployed and running:\n\n```\noc get all -l app=service-a -n default\noc get all -l app=service-b -n default\nNAME                             READY   STATUS    RESTARTS   AGE\npod/service-a-799d4dc5f8-v7l74   2/2     Running   0          19m\npod/service-b-5c45ff88d-dr7cl   2/2     Running   0          23m\n\nNAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/service-a   ClusterIP   172.30.243.210   <none>        9080/TCP   19m\nservice/service-b   ClusterIP   172.30.40.248   <none>        9081/TCP   23m\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/service-a   1/1     1            1           19m\ndeployment.apps/service-b   1/1     1            1           23m\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/service-a-799d4dc5f8   1         1         1       19m\nreplicaset.apps/service-b-5c45ff88d   1         1         1       23m\n```\n\nNotice that under the `READY` column for pods, it shows that there are two (2/2) containers running, one of them is the istio sidecar proxy.\n\n5. Get the hostname for the Istio ingress gateway\n\n```\noc get route -n istio-system istio-ingressgateway \nNAME                   HOST/PORT                                            PATH   SERVICES               PORT   TERMINATION   WILDCARD\nistio-ingressgateway   istio-ingressgateway-istio-system.apps-crc.testing          istio-ingressgateway   8080                 None\n```\n\n. Use curl or open a browser with the endpoint URL using the HOST/PORT of the route\n\n```\ncurl http://istio-ingressgateway-istio-system.apps-crc.testing/sayHello/Carlos\n```\n\nNotice in the output that the message was formatted by service-b\n\n```\nHello, from service-b Carlos!\n```\n\nFrom the result, you can see that `service-a` calls `service-b` and replies back.\n\n6. In the Jaeger UI select `istio-ingressgateway` or `service-a` and click **Find Traces**\n\n![istio-trace](../../images/istio-ol-jaeger-traces.png)\n\nYou can see 7 Spans in a single trace starting from the `istio-ingressgateway` ending in `service-b.default`\n\n7. Click on one of the traces and expand the spans in the trace\n\n![istio-spans](../../images/istio-ol-jaeger-spans.png)\n\nCheck one of the labs [Lab Jaeger - Node.js](./lab1) or [Lab Jaeger - Open Liberty](./lab3) for a more in-depth lab for Opentracing with Jaeger.\n\n8. In the Kiali UI select Graph to see a topology view of the services, you can enable traffic animation under Display to see the flow of http requests\n\n![istio-kiali](../../images/istio-ol-kiali.png)\n\n9. In the Grafana UI select the Dashboard *Istio Workload Dashboard* or *Istio Service Dashboard* to see monitoring and metrics data for your services\n\n![istio-grafana](../../images/istio-ol-grafana.png)","frontmatter":{"title":"Using Jaeger in Service Mesh(Istio) with Open Liberty"},"fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/learning-cloudnative-101/src/pages/electives/dist-trace/activities/lab7/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}